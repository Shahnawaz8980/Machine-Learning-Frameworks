{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_demo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKcPgba9ySsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJucD-BSzHx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First, we should declare a variable containing the size of the training set we want to generate.\n",
        "observations = 100000\n",
        "\n",
        "# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.\n",
        "# We have picked x and z, since it is easier to differentiate them.\n",
        "# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).\n",
        "# The size of xs and zs is observations x 1. In this case: 1000 x 1.\n",
        "xs = np.random.uniform(low=-10, high=10, size=(observations,1))\n",
        "zs = np.random.uniform(-10, 10, (observations,1))\n",
        "\n",
        "# Combine the two dimensions of the input into one input matrix. \n",
        "# This is the X matrix from the linear model y = x*w + b.\n",
        "# column_stack is a Numpy method, which combines two matrices (vectors) into one.\n",
        "generated_inputs = np.column_stack((xs,zs))\n",
        "\n",
        "# We add a random small noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>\n",
        "noise = np.random.uniform(-1, 1, (observations,1))\n",
        "\n",
        "# Produce the targets according to our f(x,z) = 2x - 3z + 5 + noise definition.\n",
        "# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.\n",
        "generated_targets = 2*xs - 3*zs + 5 + noise\n",
        "\n",
        "# save into an npz file called \"TF_intro\"\n",
        "np.savez('TF_intro', inputs=generated_inputs, targets=generated_targets)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFQ37akbzv6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size =2\n",
        "output_size =1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j-8KITU1gqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we define a basic TensorFlow object - the placeholder.\n",
        "# As before, we will feed the inputs and targets to the model. \n",
        "# In the TensorFlow context, we feed the data to the model THROUGH the placeholders. \n",
        "# The particular inputs and targets are contained in our .npz file.\n",
        "\n",
        "# The first None parameter of the placeholders' shape means that\n",
        "# this dimension could be of any length. That's since we are mainly interested in\n",
        "# the input size, i.e. how many input variables we have and not the number of samples (observations)\n",
        "# The number of input variables changes the MODEL itself, while the number of observations doesn't.\n",
        "# Remember that the weights and biases were independent of the number of samples, so the MODEL is independent.\n",
        "# Important: NO calculation happens at this point.\n",
        "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
        "targets = tf.placeholder(tf.float32, [None, output_size])\n",
        "\n",
        "# As before, we define our weights and biases.\n",
        "# They are the other basic TensorFlow object - a variable.\n",
        "# We feed data into placeholders and they have a different value for each iteration\n",
        "# Variables, however, preserve their values across iterations.\n",
        "# To sum up, data goes into placeholders; parameters go into variables.\n",
        "\n",
        "# We use the same random uniform initialization in [-0.1,0.1] as in the minimal example but using the TF syntax\n",
        "# Important: NO calculation happens at this point.\n",
        "weights = tf.Variable(tf.random_uniform([input_size, output_size], minval=-0.1, maxval=0.1))\n",
        "biases = tf.Variable(tf.random_uniform([output_size], minval=-0.1, maxval=0.1))\n",
        "\n",
        "# We get the outputs following our linear combination: y = xw + b\n",
        "# Important: NO calculation happens at this point.\n",
        "# This line simply tells TensorFlow what rule to apply when we feed in the training data (below).\n",
        "outputs = tf.matmul(inputs, weights) + biases"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHLjW3xm1-M_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Again, we use a loss function, this time readily available, though.\n",
        "# mean_squared_error is the scaled L2-norm (per observation)\n",
        "# We divide by two to follow our earlier definitions. That doesn't really change anything.\n",
        "mean_loss = tf.losses.mean_squared_error(labels=targets, predictions=outputs) / 2.\n",
        "\n",
        "# Note that there also exists a function tf.nn.l2_loss. \n",
        "# tf.nn.l2_loss calculates the loss over all samples, instead of the average loss per sample.\n",
        "# Practically it's the same, a matter of preference.\n",
        "# The difference would be a smaller or larger learning rate to achieve the exact same result.\n",
        "\n",
        "# Instead of implementing Gradient Descent on our own, in TensorFlow we can simply state\n",
        "# \"Minimize the mean loss by using Gradient Descent with a given learning rate\"\n",
        "# Simple as that.\n",
        "optimize = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(mean_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCxbL_whrRdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgVARBFt3oPn",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-YHcYJgufV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "bdfac2eb-ca7a-4aec-f118-1f3652508b46"
      },
      "source": [
        "# So far we've defined the placeholders, variables, the loss function and the optimization method.\n",
        "# We have the structure for training, but we haven't trained anything yet.\n",
        "# The actual training (and subsequent implementation of the ML algorithm) happens inside sessions.\n",
        "sess = tf.InteractiveSession()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45C08YTjvAME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Before we start training, we need to initialize our variables: the weights and biases.\n",
        "# There is a specific method for initializing called global_variables_initializer().\n",
        "# Let's declare a variable \"initializer\" that will do that.\n",
        "initializer = tf.global_variables_initializer()\n",
        "\n",
        "# Time to initialize the variables.\n",
        "sess.run(initializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOx4XXf8wVdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We finally load the training data we created above.\n",
        "training_data = np.load('TF_intro.npz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGcCD8iQ0ZpY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab27a1dd-313e-488e-9aaf-ccb98a95f7db"
      },
      "source": [
        "# As in the previous example, we train for a set number (100) of iterations over the dataset\n",
        "for i in range(100):\n",
        "    # This expression is a bit more complex but you'll learn to appreciate its power and\n",
        "    # flexibility in the following lessons.\n",
        "    # sess.run is the session's function to actually do something, anything.\n",
        "    # Above, we used it to initialize the variables.\n",
        "    # Here, we use it to feed the training data to the computational graph, defined by the feed_dict parameter\n",
        "    # and run operations (already defined above), given as the first parameter (optimize, mean_loss).\n",
        "    \n",
        "    # So the line of code means: \"Run the optimize and mean_loss operations by filling the placeholder\n",
        "    # objects with data from the feed_dict parameter\".\n",
        "    # Curr_loss catches the output from the two operations.\n",
        "    # Using \"_,\" we omit the first one, because optimize has no output (it's always \"None\"). \n",
        "    # The second one catches the value of the mean_loss for the current run, thus curr_loss actually = mean_loss \n",
        "    _, curr_loss = sess.run([optimize, mean_loss], \n",
        "        feed_dict={inputs: training_data['inputs'], targets: training_data['targets']})\n",
        "    \n",
        "    # We print the current average loss\n",
        "    print(curr_loss)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "224.3692\n",
            "107.13951\n",
            "54.59339\n",
            "30.96922\n",
            "20.27844\n",
            "15.372545\n",
            "13.05551\n",
            "11.898395\n",
            "11.262275\n",
            "10.861455\n",
            "10.568148\n",
            "10.325183\n",
            "10.106948\n",
            "9.901953\n",
            "9.705012\n",
            "9.513771\n",
            "9.327128\n",
            "9.144556\n",
            "8.965775\n",
            "8.790625\n",
            "8.618991\n",
            "8.450785\n",
            "8.285936\n",
            "8.124368\n",
            "7.9660177\n",
            "7.8108163\n",
            "7.658706\n",
            "7.5096235\n",
            "7.363505\n",
            "7.2202964\n",
            "7.079938\n",
            "6.9423714\n",
            "6.80754\n",
            "6.6753964\n",
            "6.54588\n",
            "6.418941\n",
            "6.294527\n",
            "6.17259\n",
            "6.0530787\n",
            "5.9359493\n",
            "5.8211465\n",
            "5.7086287\n",
            "5.5983515\n",
            "5.490268\n",
            "5.3843365\n",
            "5.280512\n",
            "5.1787524\n",
            "5.07902\n",
            "4.9812713\n",
            "4.8854675\n",
            "4.7915707\n",
            "4.699542\n",
            "4.6093435\n",
            "4.5209417\n",
            "4.4342995\n",
            "4.34938\n",
            "4.266151\n",
            "4.1845775\n",
            "4.1046276\n",
            "4.02627\n",
            "3.949471\n",
            "3.8741992\n",
            "3.8004255\n",
            "3.7281203\n",
            "3.657254\n",
            "3.5877979\n",
            "3.5197244\n",
            "3.4530046\n",
            "3.387614\n",
            "3.3235228\n",
            "3.2607076\n",
            "3.1991432\n",
            "3.1388044\n",
            "3.079665\n",
            "3.0217025\n",
            "2.964893\n",
            "2.909215\n",
            "2.8546453\n",
            "2.8011596\n",
            "2.7487402\n",
            "2.6973631\n",
            "2.6470082\n",
            "2.5976565\n",
            "2.5492854\n",
            "2.501878\n",
            "2.4554126\n",
            "2.4098737\n",
            "2.3652403\n",
            "2.3214936\n",
            "2.2786198\n",
            "2.2365975\n",
            "2.1954122\n",
            "2.1550462\n",
            "2.115483\n",
            "2.076708\n",
            "2.0387046\n",
            "2.0014565\n",
            "1.9649503\n",
            "1.92917\n",
            "1.8941029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhwrvVHj1abM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "77829bee-04e5-42b7-cd1f-106500f1ee2d"
      },
      "source": [
        "# As before, we want to plot the last output vs targets after the training is supposedly over.\n",
        "# Same notation as above but this time we don't want to train anymore, and we are not interested\n",
        "# in the loss function value.\n",
        "# What we want, however, are the outputs. \n",
        "# Therefore, instead of the optimize and mean_loss operations, we pass the \"outputs\" as the only parameter.\n",
        "out = sess.run([outputs], \n",
        "               feed_dict={inputs: training_data['inputs']})\n",
        "# The model is optimized, so the outputs are calculated based on the last form of the model\n",
        "\n",
        "# We have to np.squeeze the arrays in order to fit them to what the plot function expects.\n",
        "# Doesn't change anything as we cut dimensions of size 1 - just a technicality.\n",
        "plt.plot(np.squeeze(out), np.squeeze(training_data['targets']))\n",
        "plt.xlabel('outputs')\n",
        "plt.ylabel('targets')\n",
        "plt.show()\n",
        "        \n",
        "# Voila - what you see should be exactly the same as in the previous notebook!\n",
        "# You probably don't see the point of TensorFlow now - it took us more lines of code\n",
        "# to achieve this simple result. However, once we go deeper in the next chapter,\n",
        "# TensorFlow will save us hundreds of lines of code."
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBCq6_mS2x3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}